{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing liberarys part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # sending requests for html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # webscraping and handeling html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re, time, random,urllib # other things we need to work with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collacting all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html\" # it countains 10000 film urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html\" # it countains 10000 film urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies3.html\" # it countains 10000 film urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Three_url_list = [url1, url2, url3] #this list countains three urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movies_list = [] # this list will have 30,000 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_all_links(URL):\n",
    "    \"\"\" This function will find and all urls and append them into a list \"\"\"\n",
    "    response = requests.get(URL) # geting response from website\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\") # webscraping in html form\n",
    "    for link in soup.find_all(\"a\", href = True): # will find all anchore tages in the page\n",
    "        Movies_list.append(link[\"href\"]) # will append urls into Move_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in Three_url_list: # send one by one three main urls to function\n",
    "    Generate_all_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Movies_list)# if you can see 30000 url in list it means you have successfully gathered all urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download HTML files part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of downloading 30000 html fiels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Python_and_data\\GIT_projects\\search_engine\\All_HTML_fiels\n"
     ]
    }
   ],
   "source": [
    "cd E:/Python/Python_and_data/GIT_projects/search_engine/Html_fiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:/Python/Python_and_data/GIT_projects/search_engine/Html_fiels/number_check.txt\",\"r\") as read_num: # because of large size we need to download and time that will take it's better to have memory wich can show us were in wich part of the great download and so we can countineu from that part if anything went wrong so that we dont need to start from Article0.\n",
    "    CHECK_past = read_num.read() # reading the number from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in range(int(CHECK_past),len(Movies_list)): # here we have mechanism wich will start from number in the text file and looping from where it was intrubted\n",
    "    \n",
    "    ## check page status  and delay part##\n",
    "    URL = Movies_list[movie] # get url form list\n",
    "    Check_blouck = requests.get(URL) # get response from wikipedia page\n",
    "    if int(Check_blouck.status_code) == 200: # if page is avalibul have 1 or 2 or 3 secounds delay then start opration \n",
    "        time.sleep(random.randint(1,2))\n",
    "    elif int(Check_blouck.status_code) == 429: # if wikipedia block our ip addres dealy for 20 minuts then start opration\n",
    "        time.sleep(1200) \n",
    "        \n",
    "    ## opration part ##\n",
    "    if int(Check_blouck.status_code) == 200: # here we need to check that we dont face with 404 and everithng is okey to start opration\n",
    "        URL = Movies_list[movie] # get url form list\n",
    "        Response = urllib.request.urlopen(URL) # get response from page and save it for reading pagr with using urllib library\n",
    "        Web_countent = Response.read() # reading detaile of page\n",
    "        Final_file = open(\"Article_{0}.html\".format(movie),\"wb\") # open a html file for write page details in it\n",
    "        Final_file.write(Web_countent) # write page details in html file\n",
    "        Final_file.close() # close file\n",
    "        with open(\"E:/Python/Python_and_data/GIT_projects/search_engine/Html_fiels/number_check.txt\",\"w\") as write_num: # now that we successfully finished opration we need to send its number to file(it means we have downloaded up to this number from 30000)\n",
    "            WRITE_NUMBER = write_num.write(str(movie))\n",
    "    else:\n",
    "        with open(\"E:/Python/Python_and_data/GIT_projects/search_engine/Html_fiels/number_check.txt\",\"w\") as write_num: # if file has a problem we will pass it and write its number to text file\n",
    "            WRITE_NUMBER = write_num.write(str(movie))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect data from html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Python_and_data\\GIT_projects\\search_engine\\Html_fiels\n"
     ]
    }
   ],
   "source": [
    "cd E:\\Python\\Python_and_data\\GIT_projects\\search_engine\\Html_fiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_files_data(Num):\n",
    "    \"\"\"This function will collect information from html files\"\"\"\n",
    "    open_HTML_file = open(\"Article_{0}.html\".format(Num),\"r\",encoding=\"utf-8\") # opening html file\n",
    "    soup = BeautifulSoup(open_HTML_file,\"html.parser\") # use soup to read info in html parser form\n",
    "    \n",
    "    ## Title part ##\n",
    "    title = soup.find(\"h1\") # h1 is the biggest fount in html and titles are the only one wich use h1\n",
    "    Title = title.text # text the title\n",
    "    \n",
    "    ## intro and plot ##\n",
    "    list1 = [] # empty list\n",
    "    for element in soup.find_all([\"h2\",\"p\"]): # if we want to find intro and plot part we need to search for h2 and p\n",
    "        list1.append(element.text) # append those elements wich matched in to empty list\n",
    "        \n",
    "    ## intro part ##\n",
    "    ## intro part in this senario can be find in two position ## \n",
    "    if len(list1[0]) == 1: # check for first location \n",
    "            Intro = list1[1] # if frist location was empty we will select secound location\n",
    "    else: \n",
    "        Intro = list1[0] # else we will select first location\n",
    "        if \"This article needs additional citations\" in Intro:\n",
    "            Intro = \"NA\"\n",
    "        \n",
    "    ## specify contents ##\n",
    "    \n",
    "    ## here we will try to find plot part with content table elements\n",
    "    try:\n",
    "        list2 = [] # this lsit will countains contents table elements \n",
    "        for i in soup.find_all(\"span\"): # we use span to specify those elements\n",
    "            list2.append(i.text) # append it to the list\n",
    "        a = list2.index('1') # wikipedia content table first element  is plot\n",
    "        Start_plot = list2[a+1] # we will set start point after heading ======> for example (Plot[edit])\n",
    "        b = list2.index('2') # wikipedia content table secound element  is other element after plot\n",
    "        Finish_plot = list2[b+1] # we will set finish point after heading ======> for example (summery[edit])\n",
    "        # we actually need those texts between start point and finish point\n",
    "        ## plot part ##\n",
    "        for point in list1: #looping in list1 for plot part\n",
    "            if point == Start_plot+\"[edit]\": # we will plus [edit] to Start point\n",
    "                Start = list1.index(point) # we need to find its position in list\n",
    "            if point == Finish_plot+\"[edit]\": #  we will plus [edit] to end point\n",
    "                Finish = list1.index(point) # we need to find its position in list\n",
    "        Plot = list1[Start+1:Finish] # plot part is now in Plot list\n",
    "    except: # some times wiki page doesn't have content table \n",
    "        list2 = [] # we will append all h2 headings in list2\n",
    "        for H2 in soup.find_all(\"h2\"): # find all h2 in html file\n",
    "            list2.append(H2.text) # apped H2 in list2\n",
    "        try: # here we will face with two senario: 1-plot part exist 2-plot part dosn't exist\n",
    "            # 1st senario:\n",
    "            Start_plot = list2[0] # first element of list2 is start point of plot\n",
    "            Finish_plot = lis2[1] # seconde element of list2 is finish point of plot\n",
    "            for point in list1: #looping in list1 for plot part\n",
    "                if point == Start_plot: # start point is start point of the plot text\n",
    "                    Start = list1.index(point) # we need to find its position in list\n",
    "                if point == Finish_plot: # finish point is finish point of the plot text\n",
    "                    Finish = list1.index(point) # we need to find its position in list\n",
    "            Plot = list1[Start+1:Finish] # plot text\n",
    "            # 2nd senario:\n",
    "        except:\n",
    "            Plot = \"NA\" # we will set Plot as NA \n",
    "            pass\n",
    "    ## information table part ##\n",
    "    # in some wiki pages we dont have information table so we will face with two senario:  1-info table exist 2-info table dosn't exist\n",
    "    # 1st senario:\n",
    "    try:\n",
    "        list3 = [] # this list will have all trs in tbody \n",
    "        table = soup.find(\"table\",{\"class\":\"infobox vevent\"}) # we will search specifeid table (infobox vevent)\n",
    "        tbody = table.find(\"tbody\") # find table part in table\n",
    "        for tr in tbody.find_all(\"tr\"): # find all trs\n",
    "            list3.append(tr.text) # append all trs to list3\n",
    "        \n",
    "        Film_name = list3[0] # first element is movie name\n",
    "        # by defult we will set all of infos as NA if we find value for them it will change\n",
    "        Directed_by = \"NA\"\n",
    "        Produced_by = \"NA\"\n",
    "        Written_by = \"NA\"\n",
    "        Starring = \"NA\"\n",
    "        Music_by = \"NA\"\n",
    "        Release_date = \"NA\"\n",
    "        Running_time = \"NA\"\n",
    "        Country = \"NA\"\n",
    "        Language = \"NA\"\n",
    "        Budget = \"NA\" \n",
    "        \n",
    "        for find_info in range(0,len(list3)): # looping in list3 for trs\n",
    "            Info_element = list3[find_info] \n",
    "            \n",
    "            if \"Directed by\" in Info_element: # if Directed by is in Info_element\n",
    "                Splited_info = Info_element.split(\"Directed by\") # we will use Directed by as key word to split text\n",
    "                Directed_by = Splited_info[1] # 2nd element of Splited_info is Director information\n",
    "            \n",
    "            if \"Produced by\" in Info_element:# if Produced by is in Info_element\n",
    "                Splited_info = Info_element.split(\"Produced by\") # we will use Produced by as key word to split text\n",
    "                Produced_by = Splited_info[1] #2nd element of Splited_info is Producer information\n",
    "            \n",
    "            if \"Written by\" in Info_element:# if Written by is in Info_element\n",
    "                Splited_info = Info_element.split(\"Written by\")# we will use Written by as key word to split text\n",
    "                Written_by = Splited_info[1] #2nd element of Splited_info is writer information\n",
    "            \n",
    "            if \"Starring\" in Info_element:# if Starring is in Info_element\n",
    "                Splited_info = Info_element.split(\"Starring\")# we will use Starring as key word to split text\n",
    "                Starring = Splited_info[1]#2nd element of Splited_info is Starring information\n",
    "        \n",
    "            if \"Music by\" in Info_element:# if Music by is in Info_element\n",
    "                Splited_info = Info_element.split(\"Music by\")# we will use Music by as key word to split text\n",
    "                Music_by = Splited_info[1]#2nd element of Splited_info is Music by information\n",
    "            \n",
    "            if \"Release date\" in Info_element:# if Release date is in Info_element\n",
    "                Splited_info = Info_element.split(\"Release date\")# we will use Release date as key word to split text\n",
    "                Release_date = Splited_info[1]#2nd element of Splited_info is Release date information\n",
    "            \n",
    "            if \"Running time\" in Info_element:# if Running time is in Info_element\n",
    "                Splited_info = Info_element.split(\"Running time\")# we will use Running time as key word to split text\n",
    "                Running_time = Splited_info[1]#2nd element of Splited_info is Running time information\n",
    "            \n",
    "            if \"Country\" in Info_element:# if Country is in Info_element\n",
    "                Splited_info = Info_element.split(\"Country\")# we will use Country as key word to split text\n",
    "                Country = Splited_info[1]#2nd element of Splited_info is Country information\n",
    "            \n",
    "            if \"Language\" in Info_element:# if Language is in Info_element\n",
    "                Splited_info = Info_element.split(\"Language\")# we will use Language as key word to split text\n",
    "                Language = Splited_info[1]#2nd element of Splited_info is Language information\n",
    "        \n",
    "            if \"Budget\" in Info_element:# if Budget is in Info_element\n",
    "                Splited_info = Info_element.split(\"Budget\")# we will use Budget as key word to split text\n",
    "                Budget = Splited_info[1]#2nd element of Splited_info is Budget information\n",
    "                \n",
    "            # write tsv file    \n",
    "            with open (\"E:\\Python\\Python_and_data\\GIT_projects\\search_engine\\TSV_files\\Movie{0}.tsv\".format(Num),\"w\") as tsv_output:\n",
    "                tsv_writer = csv.writer(tsv_output, delimiter='\\t')\n",
    "                tsv_writer.writerow([Title,Intro,Plot,Film_name,Directed_by,Produced_by,Written_by,Starring,Music_by,Release_date,Running_time,Country,Language,Budget])\n",
    "    except: # 2nd senario:\n",
    "        Film_name = \"NA\"\n",
    "        Directed_by = \"NA\"\n",
    "        Produced_by = \"NA\"\n",
    "        Written_by = \"NA\"\n",
    "        Starring = \"NA\"\n",
    "        Music_by = \"NA\"\n",
    "        Release_date = \"NA\"\n",
    "        Running_time = \"NA\"\n",
    "        Country = \"NA\"\n",
    "        Language = \"NA\"\n",
    "        Budget = \"NA\"\n",
    "        with open (\"E:\\Python\\Python_and_data\\GIT_projects\\search_engine\\TSV_files\\Movie{0}.tsv\".format(Num),\"wt\",encoding=\"utf-8\") as tsv_output:\n",
    "                tsv_writer = csv.writer(tsv_output, delimiter='\\t')\n",
    "                tsv_writer.writerow([Title,Intro,Plot,Film_name,Directed_by,Produced_by,Written_by,Starring,Music_by,Release_date,Running_time,Country,Language,Budget])\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,30000):\n",
    "    try:\n",
    "        html_files_data(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Python_and_data\\GIT_projects\\search_engine\\TSV_files\n"
     ]
    }
   ],
   "source": [
    "cd E:\\Python\\Python_and_data\\GIT_projects\\search_engine\\TSV_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize , sent_tokenize # for tokenizing words\n",
    "from nltk.corpus import stopwords # for stop words\n",
    "from nltk.stem import PorterStemmer # for stemming\n",
    "import string # for working with strings (in this case for punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_language_process(Number):\n",
    "    with open(\"Movie{0}.tsv\".format(Number), \"r\", encoding=\"ISO-8859-1\") as Read_TSV:\n",
    "        Data = Read_TSV.read()\n",
    "        Data = Data.split(\"\\t\")\n",
    "    Intro = Data[1]\n",
    "    Plot = Data[2]\n",
    "    \n",
    "        ## tokenizing ##\n",
    "    token_intro = word_tokenize(Intro)\n",
    "    token_plot = word_tokenize(Plot)\n",
    "    \n",
    "    ## Remove Stop words ##\n",
    "    RSW_list_intro = []\n",
    "    RSW_list_plot = []\n",
    "    Stop_words = set(stopwords.words(\"english\"))\n",
    "    #intro part\n",
    "    for SW in token_intro:\n",
    "        if SW not in Stop_words:\n",
    "            RSW_list_intro.append(SW)\n",
    "    # plot part         \n",
    "    for SW in token_plot:\n",
    "        if SW not in Stop_words:\n",
    "            RSW_list_plot.append(SW)\n",
    "            \n",
    "    \n",
    "    ## Stemming part ##\n",
    "    Stemm_list_intro = []\n",
    "    Stemm_list_plot = []\n",
    "    PS = PorterStemmer()\n",
    "    # intro part\n",
    "    for stemming_element in RSW_list_intro:\n",
    "        Stemm_list_intro.append(PS.stem(stemming_element))\n",
    "    # plot part\n",
    "    for stemming_element in RSW_list_plot:\n",
    "        Stemm_list_plot.append(PS.stem(stemming_element))\n",
    "\n",
    "    ## Remove punctuation ##\n",
    "    Remove_punctuation_intro = []\n",
    "    Remove_punctuation_plot = []\n",
    "    punctuation_elements = string.punctuation\n",
    "    # intro part\n",
    "    for RP in Stemm_list_intro:\n",
    "        if RP not in punctuation_elements:\n",
    "            Remove_punctuation_intro.append(RP)\n",
    "    # plot part\n",
    "    for RP in Stemm_list_plot:\n",
    "        if RP not in punctuation_elements:\n",
    "            Remove_punctuation_plot.append(RP)\n",
    "    \n",
    "    ## anything else ##\n",
    "    \n",
    "    ## number filter\n",
    "    Filter_1_intro = []\n",
    "    Filter_1_plot = []\n",
    "    nums = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    # intro part\n",
    "    for AE in Remove_punctuation_intro:\n",
    "        if AE not in nums:\n",
    "            Filter_1_intro.append(AE)\n",
    "    # plot part\n",
    "    for AE in Remove_punctuation_plot:\n",
    "        if AE not in nums:\n",
    "            Filter_1_plot.append(AE)\n",
    "            \n",
    "    ## opostrophs filter\n",
    "    Filter_2_intro = []\n",
    "    Filter_2_plot = []\n",
    "    opostroph = [\"'S\",\"'s\",\"'m\",\"'re\",\"'t\"]\n",
    "    # intro part\n",
    "    for s in Filter_1_intro:\n",
    "        if s not in opostroph:\n",
    "            Filter_2_intro.append(s)\n",
    "    # plot part\n",
    "    for s in Filter_1_plot:\n",
    "        if s not in opostroph:\n",
    "            Filter_2_plot.append(s)\n",
    "            \n",
    "    ## \\\\n part        \n",
    "    joined_intro = \" \".join(Filter_2_intro)\n",
    "    joined_plot = \" \".join(Filter_2_plot)\n",
    "    \n",
    "    rr = joined_intro.split(\".\\\\n\")\n",
    "    ff = joined_plot.split(\".\\\\n\")\n",
    "    \n",
    "    ## \\\\\n",
    "    joined_intro = \" \".join(rr)\n",
    "    joined_plot = \" \".join(ff)\n",
    "    \n",
    "    rr = joined_intro.split(\"\\\\\")\n",
    "    ff = joined_plot.split(\"\\\\\")\n",
    "    \n",
    "    ## unicodes: \\x94 , \\x92 , ...\n",
    "    joined_intro = \" \".join(rr)\n",
    "    joined_plot = \" \".join(ff)\n",
    "    \n",
    "    # intro part\n",
    "    printable = set(string.printable)\n",
    "    intro_ascii = ''.join(filter(lambda x: x in printable, joined_intro))\n",
    "    # plot part\n",
    "    printable = set(string.printable)\n",
    "    plot_ascii = ''.join(filter(lambda x: x in printable, joined_plot))\n",
    "    \n",
    "    asembeled_intro = intro_ascii.split(\" \")\n",
    "    asembeled_plot = plot_ascii.split(\" \")\n",
    "    \n",
    "    # remove elements that didn't exist in string.punctuation\n",
    "    \n",
    "    Filter_3_intro = []\n",
    "    Filter_3_plot = []\n",
    "    PATERN = [\"''\",\"``\",'\"\"','',\"...\"]\n",
    "    # intro part\n",
    "    for s in asembeled_intro:\n",
    "        if s not in PATERN:\n",
    "            Filter_3_intro.append(s)\n",
    "    # plot part\n",
    "    for s in asembeled_plot:\n",
    "        if s not in PATERN:\n",
    "            Filter_3_plot.append(s)\n",
    "    print(Filter_3_intro)\n",
    "    print(Filter_3_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['con', 'air', '1997', 'american', 'action', 'crime', 'film', 'direct', 'simon', 'west', 'written', 'scott', 'rosenberg', 'produc', 'jerri', 'bruckheim', 'the', 'film', 'star', 'nicola', 'cage', 'john', 'cusack', 'john', 'malkovich', 'steve', 'buscemi', 'ving', 'rhame', 'colm', 'meaney', 'mykelti', 'williamson', 'dave', 'chappel', 'rachel', 'ticotin', 'danni', 'trejo', 'nick', 'chinlund', 'jess', 'borrego', 'jose', 'zuniga', 'monica', 'potter', 'support', 'roles.']\n",
      "[\"'honor\", 'discharg', 'armi', 'ranger', 'sergeant', 'cameron', 'poe', 'return', 'home', 'pregnant', 'wife', 'tricia', 'seri', 'combat', 'deploy', 'howev', 'sentenc', 'ten', 'year', 'prison', 'inadvert', 'kill', 'drunken', 'man', 'one', 'three', 'tri', 'attack', 'tricia', 'while', 'prison', 'poe', 'commun', 'tricia', 'newborn', 'daughter', 'casey', 'letter', 'eight', 'year', 'later', 'parol', 'board', 'flight', 'alabama', 'board', 'jailbird', 'fairchild', 'c-123', 'provid', 'convert', 'fli', 'prison', 'transport', 'poe', 'accompani', 'diabet', 'cellmat', 'friend', 'mike', 'baby-o', 'o', \"'del\", 'transfer', 'the', 'flight', 'overseen', 'u.', 'marshal', 'vinc', 'larkin', 'approach', 'dea', 'agent', 'duncan', 'malloy', 'willi', 'sim', 'latter', 'plan', 'go', 'undercov', 'get', 'inform', 'drug', 'lord', 'francisco', 'cindino', 'pick', 'en', 'route', 'A', 'number', 'inmat', 'transfer', 'new', 'supermax', 'prison', 'includ', 'mass', 'murder', 'william', 'billi', 'bedlam', 'bedford', 'serial', 'rapist', 'john', 'johnni', '23', 'baca', 'black', 'guerrilla', 'famili', 'member', 'nathan', 'diamond', 'dog', 'jone', 'crimin', 'mastermind', 'cyru', 'viru', 'grissom', 'after', 'take', 'affabl', 'inmat', 'joe', 'pinbal', 'parker', 'incit', 'riot', 'distract', 'releas', 'grissom', 'diamond', 'dog', 'lead', 'takeov', 'plane', 'the', 'hijack', 'inmat', 'plot', 'land', 'carson', 'airport', 'schedul', 'pick', 'transfer', 'prison', 'fli', 'non-extradit', 'countri', 'sim', 'tri', 'take', 'control', 'plane', 'grissom', 'kill', 'poe', 'grissom', 'also', 'foil', 'johnni', '23', 'rape', 'attempt', 'plane', 'femal', 'guard', 'salli', 'bishop', 'due', 'grissom', 'vocal', 'hatr', 'rapist', 'n', \"'the\", 'plane', 'arriv', 'carson', 'citi', 'schedul', 'inmat', 'exchang', 'commenc', 'ground', 'crew', 'unawar', 'hijack', 'disguis', 'guard', 'As', 'transfer', 'begin', 'plane', 'guard', 'pilot', 'forc', 'pose', 'inmat', 'convey', 'plane', 'amongst', 'new', 'passeng', 'cindino', 'new', 'pilot', 'earl', 'swamp', 'thing', 'william', 'serial', 'killer', 'garland', 'green', 'fear', 'mani', 'inmat', 'the', 'author', 'discov', 'hijack', 'upon', 'find', 'evid', 'grissom', 'old', 'cell', 'tape', 'record', 'place', 'disguis', 'guard', 'poe', 'unabl', 'stop', 'plane', 'take', 'meanwhil', 'pinbal', 'sent', 'inmat', 'dispos', 'plane', 'transpond', 'keep', 'detect', 'tri', 'fail', 'make', 'back', 'hijack', 'plane', 'the', 'inmat', 'plan', 'land', 'lerner', 'airport', 'abandon', 'airstrip', 'desert', 'transfer', 'onto', 'anoth', 'plane', 'own', 'cindino', 'cartel', 'poe', 'find', 'pinbal', 'corps', 'trap', 'land', 'gear', 'write', 'messag', 'u.', 'marshal', 'bodi', 'throw', 'larkin', 'learn', 'news', 'head', 'lerner', 'contact', 'nation', 'guard', 'bedford', 'raid', 'cargo', 'discov', 'poe', 'ident', 'read', 'parol', 'letter', 'find', 'toy', 'bunni', 'poe', 'intend', 'give', 'daughter', 'forc', 'poe', 'kill', 'him', 'the', 'jailbird', 'ground', 'lerner', 'sign', 'transfer', 'aircraft', 'poe', 'warn', 'inmat', 'cindino', 'past', 'act', 'deceit', 'betray', 'thu', 'grissom', 'order', 'other', 'fuel', 'plane', 'get', 'readi', 'takeoff', 'poe', 'leav', 'find', 'baby-o', 'syring', 'give', 'insulin', 'meet', 'larkin', 'inform', 'situat', 'the', 'duo', 'discov', 'cindino', 'plan', 'escap', 'hidden', 'privat', 'jet', 'larkin', 'sabotag', 'take', 'grissom', 'execut', 'cindino', 'ignit', 'crash', 'plane', 'fuel', 'meanwhil', 'johnni', '23', 'assign', 'sentri', 'control', 'tower', 'spot', 'nation', 'guard', 'convoy', 'approach', 'give', 'alarm', 'the', 'inmat', 'quickli', 'find', 'cach', 'fulli', 'load', 'shotgun', 'm-16', 'rifl', 'plane', 'cargo', 'hold', 'origin', 'assign', 'guard', 'prepar', 'ambush', 'As', 'nation', 'guardsmen', 'arriv', 'inmat', 'launch', 'assault', 'kill', 'larkin', 'defend', 'surviv', 'troop', 'use', 'bulldoz', 'makeshift', 'shield', 'sever', 'inmat', 'kill', 'rest', 'flee', 'back', 'onto', 'jailbird', 'take', 'flight', 'poe', 'ident', 'reveal', 'bedford', 'bodi', 'found', 'grissom', 'execut', 'baby-o', 'larkin', 'malloy', 'arriv', 'attack', 'helicopt', 'launch', 'machin', 'gun', 'fire', 'damag', 'jailbird', 'fuel', 'tank', 'though', 'larkin', 'order', 'plane', 'land', 'mccarran', 'intern', 'airport', 'swamp', 'thing', 'forc', 'land', 'la', 'vega', 'strip', 'caus', 'mass', 'destruct', 'kill', 'johnni', '23', 'cyru', 'diamond', 'dog', 'swamp', 'thing', 'escap', 'fire', 'truck', 'pursu', 'poe', 'larkin', 'polic', 'motorcycl', 'the', 'chase', 'lead', 'death', 'three', 'escape', 'cyru', 'kill', 'pile', 'driver', 'crush', 'head', 'As', 'rest', 'convict', 'quickli', 're-apprehend', 'poe', 'larkin', 'form', 'friendship', 'tricia', 'casey', 'arriv', 'poe', 'meet', 'daughter', 'first', 'time', 'give', 'toy', 'rabbit', 'bought', 'her', \"'the\", 'crimin', 'unaccount', 'garland', 'green', 'gambl', 'undisclos', 'casino', 'appar', 'reformed']\n"
     ]
    }
   ],
   "source": [
    "natural_language_process(12878)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
